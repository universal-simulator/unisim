<!doctype html>
<html lang="en-GB" class="no-js fixed-nav">
<head>
	<meta charset="UTF-8">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="format-detection" content="telephone=no">
	<meta name='robots' content='index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1' />

	<title>UniSim: Learning Interactive Real-World Simulators</title>


	<link rel='stylesheet' id='wayve-all-css' href='style.css' type='text/css' media='all' />
	<link rel='stylesheet' id='wayve-all-css' href='custom.css' type='text/css' media='all' />
	<script src="script.js" defer></script>
	<script src="bootstrap.js"></script>
	<link rel="stylesheet" href="bootstrap-grid.css">
	    
</head>

		

<article class='blog-post'>

<nav>
  <ul>
        <li><a href="#interact" data-scroll>Interactive Demos</a></li>    
        <li><a href="#action-rich" data-scroll>Action-Rich</a></li>
        <li><a href="#long-horizon" data-scroll>Long-Horizon</a></li>
        <li><a href="#diversity" data-scroll>Diversity</a></li>
        <li><a href="#planning" data-scroll>Planning</a></li>
        <li><a href="#rl" data-scroll>RL</a></li>
    </ul>
</nav>
  

<div class=blog-intro-container>
		<h1 class='too-long'>UniSim: Learning Interactive Real-World Simulators</h1>


		<div class=intro>
			<p><span style="font-weight: 400;">Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions carried out by humans, robots, and other types of interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies to training embodied agents purely in simulation that can be directly deployed in the real world. In this work, we explore these possibilities around learning a universal simulator (UniSim) of real-world interactions through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., rich labeled objects in image data, rich actions in robotics data, and rich movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move to x, y location” from otherwise static scenes and objects. Use cases for such a real-world simulator are vast. As an example, we use UniSim to simulate interactive experiences to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit significant real-world transfer from purely training in a real-world like simulator. Lastly, we show that other types of intelligence such as video captioning and detection models can also benefit from simulated experiences in UniSim, opening up even wider applications of a real-world simulator.</p>
<p><a href="materials/paper.pdf" target="_blank" rel="noopener" data-display-type="button" class="custom-button">paper</a></p>
		</div>
		<div class=blog-meta>
			<div>
							</div>
			<div>
				<span class=reading-time></span>
			</div>
		</div>
	</div>

<div class="banner-container">
  <video class=banner loop muted playsinline autoplay height=580><source src="materials/banner.mp4"></video>
</div>


    <div class="copy" >

  <div class="copy" >  
    <div class=content id="interact">
      <h2><span style="font-weight: 400;">Interactive Demos</span></h2>
      <p><span style="font-weight: 400;">Please select an initial observation and language actions you would like to use for interaction.</span></p>
    </div>
  </div>
      

<!-- Scene Selection Bar -->
<div class="scene-selection">
    <ul>
        <li><a href="#" onclick="chooseScene('kitchen', event);" title="Kitchen Scene"><video preload="metadata" src="materials/cut_carrot-final.mp4" width="150"></video></a></li>
        <li><a href="#" onclick="chooseScene('switch', event);" title="Switch Press Scene"><video preload="metadata" src="materials/press_left-final.mp4" width="150" class="faded"></video></a></li>
        <li><a href="#" onclick="chooseScene('uncover', event);" title="Uncover Scene"><video preload="metadata" src="materials/uncover_pen-final.mp4" width="150" class="faded"></video></a></li>
	<li><a href="#" onclick="chooseScene('gg', event);" title="Golden Gate Scene"><video preload="metadata" src="materials/gg_left.mp4" width="150" class="faded"></video></a></li>
	<li><a href="#" onclick="chooseScene('sc', event);" title="Sistine Chapel Scene"><video preload="metadata" src="materials/sc_left.mp4" width="150" class="faded"></video></a></li>	
    </ul>
</div>

<!-- Buttons -->
<div class="video-buttons" id="kitchen-buttons">
  <div class="step-container">
    <div class="step-label">Step 1:</div> <!-- Step label for primary buttons -->
    <ul>
      <li><a href="#" onclick="playVideo('materials/wash_hand-final.mp4', 'wash_hand', event);">Wash hands</a></li>
      <li><a href="#" onclick="playVideo('materials/pick_bowl-final.mp4', 'pick_bowl', event);">Pick up bowl</a></li>
      <li><a href="#" onclick="playVideo('materials/cut_carrot-final.mp4', 'cut_carrot', event);">Cut carrots</a></li>
      <li><a href="#" onclick="playVideo('materials/dry_hand-final.mp4', 'dry_hand', event);">Dry hands</a></li>
    </ul>
  </div>

  <div class="step-container" id="step2-container" style="display:none;">
    <div class="step-label">Step 2:</div>
    <ul id="kitchen-secondary-buttons" style="display:none;">
      <!-- Secondary buttons will be added dynamically -->
    </ul>
  </div>
</div>

<!-- Buttons for Switch Scene -->
<div class="video-buttons" id="switch-buttons" style="display: none;">
    <ul>
        <li><a href="#" onclick="playVideo('materials/press_left-final.mp4', '', event);">Press left</a></li>
        <li><a href="#" onclick="playVideo('materials/press_middle-final.mp4', '', event);">Press middle</a></li>
        <li><a href="#" onclick="playVideo('materials/press_right-final.mp4', '', event);">Press right</a></li>
        <li><a href="#" onclick="playVideo('materials/press_plug-final.mp4', '', event);">Plug in cable</a></li>
    </ul>
</div>

<!-- Buttons for Uncover Scene -->
<div class="video-buttons" id="uncover-buttons" style="display: none;">
  <div class="step-container">
    <div class="step-label">Step 1:</div> <!-- Step label for primary buttons -->  
    <ul>
      <li><a href="#" onclick="playVideo('materials/uncover_toothpaste-final.mp4', '', event);">Toothpaste</a></li>
      <li><a href="#" onclick="playVideo('materials/uncover_spider-final.mp4', 'uncover_spider', event);">Spider</a></li>
      <li><a href="#" onclick="playVideo('materials/uncover_pen-final.mp4', 'uncover_pen', event);">Pen</a></li>
      <li><a href="#" onclick="playVideo('materials/uncover_plate-final.mp4', '', event);">Plate</a></li>
      <li><a href="#" onclick="playVideo('materials/uncover_bottle-final.mp4', '', event);">Bottle</a></li>
      <li><a href="#" onclick="playVideo('materials/uncover_pickup-final.mp4', '', event);">Pickup</a></li>
    </ul>
  </div>

  <div class="step-container" id="uncover-step2-container" style="display:none;">
    <div class="step-label">Step 2:</div>
    <ul id="uncover-secondary-buttons" style="display:none;">
      <!-- Secondary buttons will be added dynamically -->
    </ul>
  </div>  
</div>

<!-- Buttons for Golden Gate -->
<div class="video-buttons" id="gg-buttons" style="display: none;">
    <ul>
      <li><a href="#" onclick="playVideo('materials/gg_left.mp4', '', event);">Left</a></li>
      <li><a href="#" onclick="playVideo('materials/gg_right.mp4', '', event);">Right</a></li>
      <li><a href="#" onclick="playVideo('materials/gg_up.mp4', '', event);">Up</a></li>
      <li><a href="#" onclick="playVideo('materials/gg_down.mp4', '', event);">Down</a></li>
      <li><a href="#" onclick="playVideo('materials/gg_zoomin.mp4', '', event);">Zoom in</a></li>
      <li><a href="#" onclick="playVideo('materials/gg_zoomout.mp4', '', event);">Zoom out</a></li>
    </ul>
</div>

<!-- Buttons for Sistine Chapel -->
<div class="video-buttons" id="sc-buttons" style="display: none;">
    <ul>
      <li><a href="#" onclick="playVideo('materials/sc_left.mp4', '', event);">Left</a></li>
      <li><a href="#" onclick="playVideo('materials/sc_right.mp4', '', event);">Right</a></li>
      <li><a href="#" onclick="playVideo('materials/sc_down.mp4', '', event);">Down</a></li>
      <li><a href="#" onclick="playVideo('materials/sc_zoomin.mp4', '', event);">Zoom in</a></li>
    </ul>
</div>


<!-- Video Display -->
<div class="single-image">
    <span>
        <video id="videoPlayer" loop muted playsinline width="3840" height="2160">
            <source src="materials/cut_carrot-final.mp4" type="video/mp4">
        </video>
    </span>
</div>
  
  
  <div class="copy" >  
    <div class=content id="action-rich">
      <h2><span style="font-weight: 400;">Action-Rich Simulations</span></h2>
      <p><span style="font-weight: 400;">One major difference between an interactive real-world simulator and typical video generation is that a simulator should support a diverse set of actions. Below, we demonstrate how UniSim can simulate different experiences given different actions starting from the same initial frame.</span></p>
    </div>
  </div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/kitchen_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/switch_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/chapel_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/golden_gate_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="long-horizon">    
    <h2><span style="font-weight: 400;">Long-Horizon Simulations</span></h2>
    <p>
      The true value of UniSim lies in simulating long episodes to enable optimizing decisions through search, planning, optimal control, or reinforcement learning. Below, we demonstrate how UniSim can simulate interactive experiences with long interaction horizons.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/fractal_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/play_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="diversity">
    <h2><span style="font-weight: 400;">Diversity and Stochasticity</span></h2>
    <p>
      UniSim can also support highly diverse and stochastic environment transitions, such as diversity in objects being revealed after removing the towel, as well as locations and colors of objects being placed, as illustrated below.
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/uncover_word.mp4"></video>
  </span>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=1060><source src="materials/put_word.mp4"></video>
  </span>
</div>


<div class="copy" >
  <div class=content id="planning">    
    <h2><span style="font-weight: 400;">Long-Horizon Planning with UniSim</span></h2>
    <p>
      UniSim can be used to train other machine intelligence such as embodied planners. We concatenate long-horizon instructions and generate videos by doing repeated rollouts in UniSim. The resulting video and instruction data can then be used to train image-goal conditioned vision-language model (VLM) policies. The simulated plans and real-robot executions are shown below.      
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/hindsight_word.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content id="rl">    
    <h2><span style="font-weight: 400;">Reinforcement Learning with UniSim</span></h2>
    <p> 
      UniSim allows effective training of RL agents <b>purely in simulation</b>, which can be directly transferred onto real robot. This can pave the way to training policies without expensive real world intervention. The simulated rollout of the RL policy is shown below:
    </p>
  </div>
</div>

<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/rl_sim_video-slow.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content>
    <p> We then deploy the RL policy trained in UniSim onto the real robot in <b>zero-shot</b>. The real-robot executions are shown below: </p>
  </div>
</div>


<div class="single-image ">
  <span>
    <video loop muted playsinline autoplay width=3840 height=2160><source src="materials/rl_real_video-fast.mp4"></video>
  </span>
</div>

<div class="copy" >
  <div class=content id="related">    
    <h2><span style="font-weight: 400;">Related Resources</span></h2>

        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/fmdm.png" class="img-fluid" alt="FMDM" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://sites.google.com/corp/view/fmdm-neurips23/" style="color: darkblue; text-decoration: none;">Foundation Models for Decision Making NeurIPS 2023 Workshop</a>
        </div>
          <div>
	    The FMDM workshop brings together the decision making community and the foundation models community in vision and language to confront the challenges in decision making at scale.
        </div>
        </div>
        </div>

	<br>
        <div class="row vspace-top">
        <div class="col-sm-3">
            <div class="move-down">
                <img src="materials/unipi.png" class="img-fluid" alt="UniPi" style="width:100%">
            </div>
        </div>
        <div class="col-sm-9">
          <div class="paper-title">
            <a href="https://universal-policy.github.io/" style="color: darkblue; text-decoration: none;">Learning Universal Policies via Text-Guided Video Generation</a>
        </div>
          <div>
	    UniPi casts sequential decision making as a text-conditioned video generation problem. UniPi produces policies that can generalize to combinatorial and multi-task environments and able to utilize broad internet-scale text-video datasets.
        </div>
        </div>
        </div>	

  </div>
</div>

    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@article{yang2023learning,
  title={Learning Interactive Real-World Simulators},
  author={Yang, Mengjiao and Du, Yilun and Dai, Bo and Ghasemipour, Kamyar and Tompson, Jonathan and Schuurmans, Dale and Abbeel, Pieter},
  journal={arXiv e-prints},
  year={2023}
}</code></pre>
    </section>


</html>


